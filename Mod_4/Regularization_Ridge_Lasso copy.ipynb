{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization \n",
    "Agenda today:\n",
    "- Reviewing overfitting & underfitting, bias variance tradeoff\n",
    "- Ridge regression \n",
    "- Lasso regression \n",
    "- AIC and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Regularizing a Model\n",
    "Even though Lasso and Ridge regressions are only used in regression, regularizing a model is a common procedure in the process of building machine learning models. It is an effectuve procedure for tackling the problem of overfitting. Generally speaking, applying regularization technique introduces some **bias** to the model, but reduces the **variance**, and therefore results in better performance in testing data. As you will see later in this module, models built from various classification algorithms often require tuning using regularization in order to overcome overfitting. \n",
    "\n",
    "What is regularization in the context of regression? As we recall, as the complexity of model increases, the model overfits and performance on the testing set decreases. Regularization techniques *shrinks* the regression coefficients such that the coefficients are not affecting the outcomes as much as they originally would have. In other words, using regularization applies a *penalty* to the coefficients of your regression model. Let's see how exactly Ridge regression and Lasso regression work to reduce variances in regression models and result in better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/26ufdipQqU2lhNA4g/giphy.gif\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Ridge Regression (L2 Norm)\n",
    "Before we dive into regularization, let's (re)visit a concept called **Cost Function**. A cost function is a measure of how good or bad the model is at estimating the relationship of our $X$ and $y$ variables. Usually, it is expressed in the difference between actual values and predicted values. For simple linear regression, the cost function is represented as:\n",
    "<center> $$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum( bx + b_0))^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression with multiple predictors, the cost function is expressed as:\n",
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2$$\n",
    "\n",
    "Where k stands for number of predictors at jth term.\n",
    "\n",
    "The ridge regression applies a penalizing parameter $\\lambda$ *slope* $^2$, such that a small bias will be introduced to the entire model depending on the value of $\\lambda$, which is called a *hyperparameter*. \n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "The result of applying such a penalizing parameter to the cost function, resulting a different regression model that minimizing the residual sum of square **and** the term $\\lambda \\sum_{j=1}^p m_j^2$. \n",
    "\n",
    "The Ridge regression improves the fit of the original regression line by introducing some bias/changing the slope and intercept of the original line. Recall the way we interpret a regression model Y = mx + b: with every unit increase in x, the outcome y increase by m unit. Therefore, the bigger the coefficient m is, the more the outcome is subjected to changes in predictor x. Ridge regression works by reducing the magnitude of the coefficient m and therefore reducing the effect the predictors have on the outcome. Let's look at a simple example.\n",
    "\n",
    "The ridge regression penalty term contains all of the coefficients squared from the original regression line except for the intercept term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Lasso Regression (L1 Norm)\n",
    "Lasso regression is very similar to Ridge regression except for one difference - the penalty term is not squared but the absolute values of the coefficients muliplied by lambda, expressed by:\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\n",
    "\n",
    "The biggest difference in Ridge and Lasso is that Lasso simultaneously performs variable selection: some coefficients are shrunk to 0, rendering them nonexistence in the original regression model. Therefore, Lasso regression performs very well when you have higher dimensional dataset where some predictors are useless; whereas Ridge works best when all the predictors are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/AWeYSE0qgpk76/giphy.gif\" width= \"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('mpg')\n",
    "\n",
    "#data = pd.read_csv(\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-2-24-09-ridge-and-lasso-regression/master/auto-mpg.csv\") \n",
    "data = data.sample(50)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"name\", \"origin\"], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644156</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.896066</td>\n",
       "      <td>0.310559</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.135065</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.187213</td>\n",
       "      <td>0.341615</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.192208</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.333115</td>\n",
       "      <td>0.335404</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.465902</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.469274</td>\n",
       "      <td>0.718361</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.462623</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.553073</td>\n",
       "      <td>0.582295</td>\n",
       "      <td>0.291925</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.145251</td>\n",
       "      <td>0.246557</td>\n",
       "      <td>0.683230</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.873115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.041558</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.121311</td>\n",
       "      <td>0.490683</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729870</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>0.841311</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511688</td>\n",
       "      <td>0.441341</td>\n",
       "      <td>0.641311</td>\n",
       "      <td>0.403727</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.553073</td>\n",
       "      <td>0.731475</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.972459</td>\n",
       "      <td>0.341615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.177377</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644156</td>\n",
       "      <td>0.581006</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910164</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.374098</td>\n",
       "      <td>0.440994</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.023377</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.139344</td>\n",
       "      <td>0.683230</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.332468</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.411475</td>\n",
       "      <td>0.465839</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.156066</td>\n",
       "      <td>0.385093</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.154426</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.197403</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.329193</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.155738</td>\n",
       "      <td>0.310559</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729870</td>\n",
       "      <td>0.513966</td>\n",
       "      <td>0.756066</td>\n",
       "      <td>0.291925</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.493770</td>\n",
       "      <td>0.602484</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111688</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.189836</td>\n",
       "      <td>0.341615</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>0.292787</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.897049</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.070130</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.157705</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.656393</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.211803</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.581006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.168852</td>\n",
       "      <td>0.844720</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.195410</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602597</td>\n",
       "      <td>0.519553</td>\n",
       "      <td>0.629836</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729870</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.788525</td>\n",
       "      <td>0.360248</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.428852</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.070130</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.147869</td>\n",
       "      <td>0.496894</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.256066</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.070130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060984</td>\n",
       "      <td>0.745342</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.329609</td>\n",
       "      <td>0.618361</td>\n",
       "      <td>0.664596</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.070130</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.162623</td>\n",
       "      <td>0.590062</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>0.087213</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.049351</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.104590</td>\n",
       "      <td>0.639752</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496894</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.110164</td>\n",
       "      <td>0.677019</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.252787</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812987</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.627541</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cylinders  displacement  horsepower    weight  acceleration  model_year\n",
       "0         1.0      0.644156    0.916201  0.896066      0.310559    0.000000\n",
       "1         0.2      0.135065    0.223464  0.187213      0.341615    0.083333\n",
       "2         1.0      0.727273    0.748603  0.934426      0.248447    0.250000\n",
       "3         0.2      0.192208    0.279330  0.333115      0.335404    1.000000\n",
       "4         0.6      0.337662    0.217877  0.465902      0.509317    0.666667\n",
       "5         1.0      0.610390    0.469274  0.718361      0.428571    0.750000\n",
       "6         0.6      0.337662    0.234637  0.462623      0.534161    0.916667\n",
       "7         1.0      0.610390    0.553073  0.582295      0.291925    0.666667\n",
       "8         0.2      0.181818    0.145251  0.246557      0.683230    0.250000\n",
       "9         1.0      0.961039    0.944134  0.873115      0.000000    0.000000\n",
       "10        0.2      0.041558    0.106145  0.121311      0.490683    0.833333\n",
       "11        1.0      0.729870    0.592179  0.841311      0.267081    0.500000\n",
       "12        1.0      0.511688    0.441341  0.641311      0.403727    0.750000\n",
       "13        1.0      0.610390    0.553073  0.731475      0.248447    0.583333\n",
       "14        1.0      0.753247    0.944134  0.972459      0.341615    0.000000\n",
       "15        0.2      0.090909    0.156425  0.177377      0.354037    0.916667\n",
       "16        1.0      0.644156    0.581006  0.573770      0.155280    0.250000\n",
       "17        1.0      1.000000    1.000000  0.910164      0.093168    0.000000\n",
       "18        0.2      0.181818    0.223464  0.374098      0.440994    1.000000\n",
       "19        0.2      0.023377    0.134078  0.139344      0.683230    0.083333\n",
       "20        0.6      0.332468    0.273743  0.411475      0.465839    0.250000\n",
       "21        0.2      0.090909    0.094972  0.156066      0.385093    1.000000\n",
       "22        0.2      0.072727    0.134078  0.154426      0.434783    0.833333\n",
       "23        0.6      0.197403    0.413408  0.420000      0.329193    0.916667\n",
       "24        0.0      0.000000    0.245810  0.155738      0.310559    0.250000\n",
       "25        1.0      0.729870    0.513966  0.756066      0.291925    0.750000\n",
       "26        0.6      0.337662    0.273743  0.493770      0.602484    0.666667\n",
       "27        0.2      0.111688    0.273743  0.189836      0.341615    0.083333\n",
       "28        0.2      0.129870    0.162011  0.292787      0.559006    0.833333\n",
       "29        1.0      0.857143    0.720670  0.897049      0.217391    0.166667\n",
       "30        0.2      0.070130    0.033520  0.157705      1.000000    1.000000\n",
       "31        0.6      0.402597    0.301676  0.656393      0.571429    0.500000\n",
       "32        0.2      0.168831    0.212291  0.211803      0.192547    1.000000\n",
       "33        1.0      0.727273    0.581006  1.000000      0.372671    0.333333\n",
       "34        0.2      0.072727    0.078212  0.168852      0.844720    0.500000\n",
       "35        0.2      0.098701    0.134078  0.195410      0.521739    1.000000\n",
       "36        1.0      0.602597    0.519553  0.629836      0.267081    0.666667\n",
       "37        1.0      0.729870    0.536313  0.788525      0.360248    0.750000\n",
       "38        0.2      0.129870    0.234637  0.428852      0.527950    0.416667\n",
       "39        0.2      0.070130    0.234637  0.147869      0.496894    0.166667\n",
       "40        0.2      0.096104    0.245810  0.256066      0.372671    0.000000\n",
       "41        0.2      0.070130    0.000000  0.060984      0.745342    0.000000\n",
       "42        0.6      0.418182    0.329609  0.618361      0.664596    0.666667\n",
       "43        0.2      0.070130    0.117318  0.162623      0.590062    0.833333\n",
       "44        0.2      0.072727    0.189944  0.087213      0.366460    0.750000\n",
       "45        0.2      0.049351    0.078212  0.104590      0.639752    0.833333\n",
       "46        0.2      0.015584    0.033520  0.000000      0.496894    0.333333\n",
       "47        0.2      0.020779    0.033520  0.110164      0.677019    0.666667\n",
       "48        0.0      0.000000    0.301676  0.252787      0.248447    0.833333\n",
       "49        1.0      0.812987    0.692737  0.627541      0.093168    0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform t`est train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[ -1.62440911  -1.54648425   7.12599961 -23.93003779   3.87518757\n",
      "   10.44885604]]\n",
      "Unpenalized Linear Regression Intercept:[25.04198053]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Coefficients are:[-8.73891885 -3.63432862 -0.         -0.          0.          6.89203904]\n",
      "Lasso Linear Regression Intercept:[25.9927846]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients are:[[-3.41261153 -5.80384519 -2.219755   -8.14665484  0.6084853   9.09814416]]\n",
      "Ridge Linear Regression Intercept:[27.28634571]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = np.reshape(ridge.predict(X_train),(40,1))\n",
    "y_h_ridge_test = np.reshape(ridge.predict(X_test),(40,1))\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_h_ridge_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31d7f20f1a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_h_ridge_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_h_ridge_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_h_ridge_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_h_ridge_train.shape)\n",
    "print(y_h_ridge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_h_lasso_train))\n",
    "print(type(y_h_ridge_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Residual for Ridge, Lasso, and Unpenalized Regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_h_ridge_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-651cd407246d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# examine the residual sum of sq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Error Ridge Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_h_ridge_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Error Ridge Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_h_ridge_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_h_ridge_train' is not defined"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Ridge and Lasso Perform in Higher Dimensional Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 degree polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 9)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try polynomial features on the regression \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate this class\n",
    "poly_2 = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "#fit and transform the data and create a  new dataframe\n",
    "df_poly= pd.DataFrame(poly_2.fit_transform(X), columns=poly_2.get_feature_names(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df_poly, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.3)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.3)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[ 6.16588371e+13 -5.20723169e-01 -6.39185523e+01 -4.70667653e+01\n",
      "   1.20435699e+02 -4.85773571e+01 -1.16578547e+01 -5.62735318e+01\n",
      "  -9.19132310e+01  6.49091296e+01  1.04620858e+02  2.58133552e+01\n",
      "   5.08599212e+01 -1.07217562e+02  3.76385690e+02  2.56615587e+01\n",
      "   2.33152307e+01  6.09470437e+01 -1.08023501e+02 -2.42675297e+02\n",
      "   5.42109745e+01  3.22614698e+01 -3.24856852e+00 -8.26824302e+01\n",
      "  -1.49834353e+02  2.36000426e+01  2.14648466e+01  1.05802867e+01]]\n",
      "Unpenalized Linear Regression Intercept:[-6.16588371e+13]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Coefficients are:[ 0.         -0.         -0.         -0.         -9.12587583  0.\n",
      "  0.         -4.19367564 -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.          7.13022275]\n",
      "Lasso Linear Regression Intercept:[24.91898262]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients are:[[ 0.         -0.9803982  -3.63091257 -3.77879131 -5.66167328 -0.2501734\n",
      "   4.31523054 -0.77916629 -0.7626552   0.07056595  0.75846227  0.77495698\n",
      "  -0.97347604 -0.15536863  0.60920708  0.7278805  -1.9579984  -3.01608773\n",
      "  -0.21301825  1.11739033 -0.94920882 -1.4509377   0.39733265 -3.74876822\n",
      "  -4.9610305  -1.09443185  1.33401142  7.30537558]]\n",
      "Ridge Linear Regression Intercept:[25.07853261]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    241.045348\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    89.390258\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    392.941799\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    100.423365\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    113.223237\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    188.006758\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even higher degree polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 462)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_5 = PolynomialFeatures(degree=5, interaction_only=False)\n",
    "#fit and transform the data and create a  new dataframe\n",
    "df_poly_5= pd.DataFrame(poly_5.fit_transform(X), columns=poly_5.get_feature_names(X.columns))\n",
    "df_poly_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df_poly_5, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    209.196038\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    95.551673\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    1056.642075\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    182.885436\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    7.860652e-25\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    1783.897995\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating AIC and BIC \n",
    "AIC and BIC are information criteria for evaluating how good of a model is by giving a measurement of parsimony and goodness of fit. \n",
    "\n",
    "- AIC is defined as: $2k - 2log(L)$\n",
    "- BIC is defined as: $klog(n) - 2log(L)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(y, y_pred, k):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    AIC = 2*k - 2*np.log(sse)\n",
    "    \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poly_5.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    910.373115\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic(y_test, y_h_lasso_test, df_poly_5.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    912.312082\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic(y_test, y_h_ridge_test, df_poly_5.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg    909.490627\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic(y_test, y_h_lin_test, df_poly_5.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
